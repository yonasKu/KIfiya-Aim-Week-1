{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yonas\\AppData\\Local\\Temp\\ipykernel_17900\\3111102901.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Total DL (Bytes)'].fillna(0, inplace=True)\n",
      "C:\\Users\\Yonas\\AppData\\Local\\Temp\\ipykernel_17900\\3111102901.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Total UL (Bytes)'].fillna(0, inplace=True)\n",
      "C:\\Users\\Yonas\\AppData\\Local\\Temp\\ipykernel_17900\\3111102901.py:82: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Dur. (ms)'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) [('Total DL (Bytes)', 'sum')] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_user_behavior\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Perform data cleaning and aggregation\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m df_user_behavior \u001b[38;5;241m=\u001b[39m \u001b[43mclean_and_aggregate_user_behavior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Task 1.2 - Exploratory Data Analysis (Summary)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexploratory_data_analysis\u001b[39m(df_user_behavior):\n",
      "Cell \u001b[1;32mIn[3], line 89\u001b[0m, in \u001b[0;36mclean_and_aggregate_user_behavior\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     86\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal UL (Bytes)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal UL (Bytes)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Perform aggregation\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m df_user_behavior \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMSISDN/Number\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSISDN/Number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_session_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDur. (ms)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTotal DL (Bytes)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_upload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTotal UL (Bytes)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_data_volume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTotal DL (Bytes)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTotal UL (Bytes)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Adding total data volume (download + upload)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m df_user_behavior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_data_volume\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_user_behavior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_download\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m df_user_behavior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_upload\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[0;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1607\u001b[0m ):\n\u001b[1;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[1;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[0;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yonas\\Desktop\\kifiya\\week-2\\Kifiya-Aim-week2\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[1;34m(self, how, obj, func)\u001b[0m\n\u001b[0;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column(s) [('Total DL (Bytes)', 'sum')] do not exist\""
     ]
    }
   ],
   "source": [
    "# Let's first load and combine the provided notebooks and Python script into a structured and streamlined code.\n",
    "# We'll begin by integrating the user overview, engagement, and experience analysis into one cohesive Python script.\n",
    "\n",
    "# Loading necessary libraries for the tasks\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Task 1 - User Overview Analysis\n",
    "\n",
    "# Data loading function from telecom_analysis.py script\n",
    "def load_data_using_sqlalchemy(query):\n",
    "    \"\"\"\n",
    "    Connects to the PostgreSQL database and loads data based on the provided SQL query using SQLAlchemy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connection string fetched from environment variables\n",
    "        connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "        # SQLAlchemy engine creation\n",
    "        engine = create_engine(connection_string)\n",
    "\n",
    "        # Load data into a pandas DataFrame\n",
    "        df = pd.read_sql_query(query, engine)\n",
    "\n",
    "        # Handle missing values by filling them with 0\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load environment variables (for connecting to PostgreSQL)\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL database parameters\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Load data from PostgreSQL database\n",
    "query = \"SELECT * FROM xdr_data;\"  # Example query, you can adjust it as needed\n",
    "df = load_data_using_sqlalchemy(query)\n",
    "\n",
    "# Check if the data was loaded successfully\n",
    "if df is not None:\n",
    "    print(\"Data loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load data.\")\n",
    "\n",
    "\n",
    "# Task 1.1 - Aggregating User Behavior Data\n",
    "def clean_and_aggregate_user_behavior(df):\n",
    "    \"\"\"\n",
    "    Clean the data by handling missing values and ensuring the necessary columns are numeric.\n",
    "    Then aggregate per user the number of sessions, session duration, download/upload data, and data volume for each application.\n",
    "    \"\"\"\n",
    "    # Convert necessary columns to numeric, forcing errors to NaN\n",
    "    df['Total DL (Bytes)'] = pd.to_numeric(df['Total DL (Bytes)'], errors='coerce')\n",
    "    df['Total UL (Bytes)'] = pd.to_numeric(df['Total UL (Bytes)'], errors='coerce')\n",
    "    df['Dur. (ms)'] = pd.to_numeric(df['Dur. (ms)'], errors='coerce')\n",
    "\n",
    "    # Handle missing values by filling NaN with 0 for numeric columns\n",
    "    df['Total DL (Bytes)'].fillna(0, inplace=True)\n",
    "    df['Total UL (Bytes)'].fillna(0, inplace=True)\n",
    "    df['Dur. (ms)'].fillna(0, inplace=True)\n",
    "\n",
    "    # Ensure there are no negative values (if necessary)\n",
    "    df['Total DL (Bytes)'] = df['Total DL (Bytes)'].abs()\n",
    "    df['Total UL (Bytes)'] = df['Total UL (Bytes)'].abs()\n",
    "\n",
    "    # Perform aggregation\n",
    "    df_user_behavior = df.groupby(\"MSISDN/Number\").agg(\n",
    "        session_count=('MSISDN/Number', 'count'),\n",
    "        total_session_duration=('Dur. (ms)', 'sum'),\n",
    "        total_download=('Total DL (Bytes)', 'sum'),\n",
    "        total_upload=('Total UL (Bytes)', 'sum'),\n",
    "        total_data_volume=(('Total DL (Bytes)', 'sum'), ('Total UL (Bytes)', 'sum'))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Adding total data volume (download + upload)\n",
    "    df_user_behavior['total_data_volume'] = df_user_behavior['total_download'] + df_user_behavior['total_upload']\n",
    "\n",
    "    return df_user_behavior\n",
    "\n",
    "# Perform data cleaning and aggregation\n",
    "df_user_behavior = clean_and_aggregate_user_behavior(df)\n",
    "\n",
    "\n",
    "# Task 1.2 - Exploratory Data Analysis (Summary)\n",
    "def exploratory_data_analysis(df_user_behavior):\n",
    "    \"\"\"\n",
    "    Conduct exploratory data analysis on the aggregated user data and return summary statistics.\n",
    "    \"\"\"\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(df_user_behavior.describe())\n",
    "\n",
    "    # Plot histograms for session duration, download, upload, and data volume\n",
    "    df_user_behavior[['total_session_duration', 'total_download', 'total_upload', 'total_data_volume']].hist(figsize=(12, 10))\n",
    "    plt.show()\n",
    "\n",
    "exploratory_data_analysis(df_user_behavior)\n",
    "\n",
    "# Task 2 - User Engagement Analysis\n",
    "# Aggregate per customer engagement metrics and normalize for k-means\n",
    "def user_engagement_analysis(df):\n",
    "    # Aggregating engagement metrics\n",
    "    engagement_metrics = df.groupby('MSISDN/Number').agg(\n",
    "        session_frequency=('MSISDN/Number', 'count'),\n",
    "        total_duration=('Dur. (ms)', 'sum'),\n",
    "        total_traffic=('Total DL (Bytes)', 'sum') + df['Total UL (Bytes)']\n",
    "    ).reset_index()\n",
    "\n",
    "    # Normalize metrics\n",
    "    scaler = StandardScaler()\n",
    "    engagement_metrics[['session_frequency', 'total_duration', 'total_traffic']] = scaler.fit_transform(\n",
    "        engagement_metrics[['session_frequency', 'total_duration', 'total_traffic']])\n",
    "\n",
    "    # K-means clustering (k=3)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    engagement_metrics['engagement_cluster'] = kmeans.fit_predict(\n",
    "        engagement_metrics[['session_frequency', 'total_duration', 'total_traffic']])\n",
    "\n",
    "    return engagement_metrics\n",
    "\n",
    "# Perform User Engagement Analysis\n",
    "df_engagement = user_engagement_analysis(df)\n",
    "print(df_engagement.head())\n",
    "\n",
    "# Task 3 - Experience Analytics\n",
    "def experience_analytics(df):\n",
    "    \"\"\"\n",
    "    Conduct user experience analysis based on network performance metrics and device characteristics.\n",
    "    \"\"\"\n",
    "    # Aggregate network performance data\n",
    "    df_experience = df.groupby('MSISDN/Number').agg(\n",
    "        avg_tcp_retransmission=('TCP DL Retrans. Vol (Bytes)', 'mean'),\n",
    "        avg_rtt=('Avg RTT DL (ms)', 'mean'),\n",
    "        avg_throughput=('Avg Bearer TP DL (kbps)', 'mean'),\n",
    "        handset_type=('Handset Type', 'first')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Handle missing values (fill with mean or mode)\n",
    "    df_experience.fillna(df_experience.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    return df_experience\n",
    "\n",
    "# Perform Experience Analytics\n",
    "df_experience = experience_analytics(df)\n",
    "print(df_experience.head())\n",
    "\n",
    "# Task 3.4 - K-Means Clustering for Experience Analysis (k=3)\n",
    "def experience_clustering(df_experience):\n",
    "    features = df_experience[['avg_throughput', 'avg_tcp_retransmission', 'avg_rtt']].copy()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "    df_experience['experience_cluster'] = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    return df_experience, kmeans.cluster_centers_\n",
    "\n",
    "df_experience_clustered, experience_cluster_centers = experience_clustering(df_experience)\n",
    "\n",
    "print(\"Experience Cluster Centers:\")\n",
    "print(pd.DataFrame(experience_cluster_centers, columns=['Avg Throughput', 'Avg TCP Retransmission', 'Avg RTT']))\n",
    "\n",
    "# Combining Task 1, 2, and 3 for an overall report\n",
    "df_combined = df_user_behavior.merge(df_engagement, on='MSISDN/Number').merge(df_experience_clustered, on='MSISDN/Number')\n",
    "\n",
    "print(\"Combined Data Head:\")\n",
    "print(df_combined.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Task 4.1 - Assign Engagement and Experience Scores\n",
    "def calculate_satisfaction_scores(df, engagement_cluster_center, experience_cluster_center):\n",
    "    # Calculate the Euclidean distance for engagement score (less engaged cluster)\n",
    "    df['engagement_score'] = euclidean_distances(\n",
    "        df[['session_frequency', 'total_duration', 'total_traffic']], [engagement_cluster_center]\n",
    "    ).flatten()\n",
    "\n",
    "    # Calculate the Euclidean distance for experience score (worst experience cluster)\n",
    "    df['experience_score'] = euclidean_distances(\n",
    "        df[['avg_throughput', 'avg_tcp_retransmission', 'avg_rtt']], [experience_cluster_center]\n",
    "    ).flatten()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use the cluster centers from previous tasks\n",
    "engagement_cluster_center = df_engagement[['session_frequency', 'total_duration', 'total_traffic']].mean(axis=0).values\n",
    "experience_cluster_center = experience_cluster_centers[2]  # Assuming the 3rd cluster is the worst experience\n",
    "\n",
    "# Assign the scores\n",
    "df_combined = calculate_satisfaction_scores(df_combined, engagement_cluster_center, experience_cluster_center)\n",
    "\n",
    "# Task 4.2 - Calculate satisfaction score as the average of engagement and experience scores\n",
    "df_combined['satisfaction_score'] = (df_combined['engagement_score'] + df_combined['experience_score']) / 2\n",
    "\n",
    "# Report top 10 satisfied customers\n",
    "top_10_satisfied_customers = df_combined.nlargest(10, 'satisfaction_score')\n",
    "print(\"Top 10 Satisfied Customers:\")\n",
    "print(top_10_satisfied_customers[['MSISDN/Number', 'satisfaction_score']])\n",
    "\n",
    "# Task 4.3 - Build a regression model to predict satisfaction score\n",
    "X = df_combined[['engagement_score', 'experience_score']]\n",
    "y = df_combined['satisfaction_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict satisfaction score\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the regression model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error of Regression: {mse}\")\n",
    "\n",
    "# Task 4.4 - Run k-means clustering (k=2) on engagement and experience scores\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "df_combined['kmeans_cluster'] = kmeans.fit_predict(df_combined[['engagement_score', 'experience_score']])\n",
    "\n",
    "# Task 4.5 - Aggregate the average satisfaction & experience score per cluster\n",
    "cluster_summary = df_combined.groupby('kmeans_cluster').agg({\n",
    "    'satisfaction_score': 'mean',\n",
    "    'experience_score': 'mean',\n",
    "    'engagement_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Cluster Summary (Average Satisfaction, Engagement, and Experience Scores):\")\n",
    "print(cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
